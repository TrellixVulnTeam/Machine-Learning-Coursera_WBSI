{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Online Class - Exercise 4 Neural Network Learning\n",
    "\n",
    "###  Instructions\n",
    "In this exercise, you will implement the backpropagation algorithm for neural\n",
    "networks and apply it to the task of hand-written digit recognition. Before\n",
    "starting on the programming exercise, we strongly recommend watching the\n",
    "video lectures and completing the review questions for the associated topics.\n",
    "To get started with the exercise, you will need to download the starter\n",
    "code and unzip its contents to the directory where you wish to complete the\n",
    "exercise. If needed, use the cd command in Octave/MATLAB to change to\n",
    "this directory before starting this exercise.\n",
    "You can also find instructions for installing Octave/MATLAB in the “En-\n",
    "vironment Setup Instructions” of the course website.\n",
    "\n",
    "```\n",
    "------------\n",
    "Files included in this exercise\n",
    "    ex4.m - Octave/MATLAB script that steps you through the exercise\n",
    "    ex4data1.mat - Training set of hand-written digits\n",
    "    ex4weights.mat - Neural network parameters for exercise 4\n",
    "    submit.m - Submission script that sends your solutions to our servers\n",
    "    displayData.m - Function to help visualize the dataset\n",
    "    fmincg.m - Function minimization routine (similar to fminunc)\n",
    "    sigmoid.m - Sigmoid function\n",
    "    computeNumericalGradient.m - Numerically compute gradients\n",
    "    checkNNGradients.m - Function to help check your gradients\n",
    "    debugInitializeWeights.m - Function for initializing weights\n",
    "    predict.m - Neural network prediction function\n",
    "    [*] sigmoidGradient.m - Compute the gradient of the sigmoid function\n",
    "    [*] randInitializeWeights.m - Randomly initialize weights\n",
    "    [*] nnCostFunction.m - Neural network cost function\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the parameters you will use for this exercise\n",
    "```\n",
    "input_layer_size  = 400;  % 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25;   % 25 hidden units\n",
    "num_labels = 10;          % 10 labels, from 1 to 10   \n",
    "                          % (note that we have mapped \"0\" to label 10)\n",
    "```                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size  = 400\n",
    "hidden_layer_size = 25\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ======== Part 1: Loading and Visualizing Data ==========\n",
    "```\n",
    "%  We start the exercise by first loading and visualizing the dataset. \n",
    "%  You will be working with a dataset that contains handwritten digits.\n",
    "%\n",
    "\n",
    "% Load Training Data\n",
    "fprintf('Loading and Visualizing Data ...\\n')\n",
    "\n",
    "load('ex4data1.mat');\n",
    "m = size(X, 1);\n",
    "\n",
    "% Randomly select 100 data points to display\n",
    "sel = randperm(size(X, 1));\n",
    "sel = sel(1:100);\n",
    "\n",
    "displayData(X(sel, :));\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "data = scipy.io.loadmat('ex4data1.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['X']\n",
    "y = data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 400)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============= Part 2: Loading Parameters =============\n",
    "```\n",
    "% In this part of the exercise, we load some pre-initialized \n",
    "% neural network parameters.\n",
    "\n",
    "fprintf('\\nLoading Saved Neural Network Parameters ...\\n')\n",
    "\n",
    "% Load the weights into variables Theta1 and Theta2\n",
    "load('ex4weights.mat');\n",
    "\n",
    "% Unroll parameters \n",
    "nn_params = [Theta1(:) ; Theta2(:)];\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = scipy.io.loadmat('ex4weights.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta1= weights['Theta1']\n",
    "Theta2= weights['Theta2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 401)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 26)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_params = np.concatenate((Theta1.ravel(), Theta2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10285,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_params.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============ Part 3: Compute Cost (Feedforward) ============\n",
    "```\n",
    "%  To the neural network, you should first start by implementing the\n",
    "%  feedforward part of the neural network that returns the cost only. You\n",
    "%  should complete the code in nnCostFunction.m to return cost. After\n",
    "%  implementing the feedforward to compute the cost, you can verify that\n",
    "%  your implementation is correct by verifying that you get the same cost\n",
    "%  as us for the fixed debugging parameters.\n",
    "%\n",
    "%  We suggest implementing the feedforward cost *without* regularization\n",
    "%  first so that it will be easier for you to debug. Later, in part 4, you\n",
    "%  will get to implement the regularized cost.\n",
    "%\n",
    "fprintf('\\nFeedforward Using Neural Network ...\\n')\n",
    "\n",
    "% Weight regularization parameter (we set this to 0 here).\n",
    "lambda = 0;\n",
    "\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
    "                   num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['Cost at parameters (loaded from ex4weights): %f '...\n",
    "         '\\n(this value should be about 0.287629)\\n'], J);\n",
    "\n",
    "fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnCostFunction import nnCostFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from ex4weights): 0.287629\n",
      "(this value should be about 0.287629)\n"
     ]
    }
   ],
   "source": [
    "lbd = 0\n",
    "\n",
    "J, grad = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                   num_labels, X, y, lbd)\n",
    "print('Cost at parameters (loaded from ex4weights): {:.6f}\\n(this value should be about 0.287629)'.format(J));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============ Part 4: Implement Regularization ============\n",
    "```\n",
    "%  Once your cost function implementation is correct, you should now\n",
    "%  continue to implement the regularization with the cost.\n",
    "%\n",
    "\n",
    "fprintf('\\nChecking Cost Function (w/ Regularization) ... \\n')\n",
    "\n",
    "% Weight regularization parameter (we set this to 1 here).\n",
    "lambda = 1;\n",
    "\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
    "                   num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['Cost at parameters (loaded from ex4weights): %f '...\n",
    "         '\\n(this value should be about 0.383770)\\n'], J);\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from ex4weights): 0.383770\n",
      "(this value should be about 0.383770)\n"
     ]
    }
   ],
   "source": [
    "lbd = 1\n",
    "J, grad = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lbd)\n",
    "\n",
    "print('Cost at parameters (loaded from ex4weights): {:.6f}\\n(this value should be about 0.383770)'.format (J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============ Part 5: Sigmoid Gradient  =============\n",
    "```\n",
    "%  Before you start implementing the neural network, you will first\n",
    "%  implement the gradient for the sigmoid function. You should complete the\n",
    "%  code in the sigmoidGradient.m file.\n",
    "%\n",
    "\n",
    "fprintf('\\nEvaluating sigmoid gradient...\\n')\n",
    "\n",
    "g = sigmoidGradient([-1 -0.5 0 0.5 1]);\n",
    "fprintf('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\\n  ');\n",
    "fprintf('%f ', g);\n",
    "fprintf('\\n\\n');\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sigmoidGradient import sigmoidGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\n",
      "  \n",
      "[0.19661193 0.23500371 0.25       0.23500371 0.19661193]\n"
     ]
    }
   ],
   "source": [
    "g = sigmoidGradient(np.array([-1, -0.5, 0, 0.5, 1]))\n",
    "print('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\\n  ')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============= Part 6: Initializing Pameters =============\n",
    "```\n",
    "%  In this part of the exercise, you will be starting to implment a two\n",
    "%  layer neural network that classifies digits. You will start by\n",
    "%  implementing a function to initialize the weights of the neural network\n",
    "%  (randInitializeWeights.m)\n",
    "\n",
    "fprintf('\\nInitializing Neural Network Parameters ...\\n')\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);\n",
    "\n",
    "% Unroll parameters\n",
    "initial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)];\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from randInitializeWeights import randInitializeWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "\n",
    "# Unroll parameters\n",
    "initial_nn_params = np.concatenate((initial_Theta1.ravel() , initial_Theta2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10285,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_nn_params.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============ Part 7: Implement Backpropagation ============\n",
    "```\n",
    "%  Once your cost matches up with ours, you should proceed to implement the\n",
    "%  backpropagation algorithm for the neural network. You should add to the\n",
    "%  code you've written in nnCostFunction.m to return the partial\n",
    "%  derivatives of the parameters.\n",
    "%\n",
    "fprintf('\\nChecking Backpropagation... \\n');\n",
    "\n",
    "%  Check gradients by running checkNNGradients\n",
    "checkNNGradients;\n",
    "\n",
    "fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checkNNGradients import checkNNGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Gradient       Analytical Gradient\n",
      "2.8383186911895564e-05   2.838318795382669e-05\n",
      "0.00015401021569161344   0.00015401021555703823\n",
      "0.00044129794485314733   0.00044129794658824615\n",
      "0.00032285838003076606   0.000322858380675971\n",
      "0.0008000839057942244    0.0008000839034693133\n",
      "0.0002456484216040167    0.00024564842185084484\n",
      "0.0003892400224358994    0.000389240023517967\n",
      "0.00017496614379552966   0.00017496614263500808\n",
      "-0.002644809895535616    -0.0026448098991406865\n",
      "-0.0002919238517584688   -0.0002919238506639142\n",
      "0.00024311816337885261   0.00024311816212281044\n",
      "0.0005546384551635697    0.0005546384578506715\n",
      "-0.002312306377483253    -0.002312306379408663\n",
      "-0.00021125534210852948  -0.00021125534212601012\n",
      "0.00034176947716346717   0.00034176947746973407\n",
      "0.0005805730163288558    0.0005805730156305033\n",
      "-0.0017960278420048326   -0.0017960278440286234\n",
      "-0.0001138126459743205   -0.00011381264664858063\n",
      "0.00041312829379691607   0.00041312829143803616\n",
      "0.0005602409847149659    0.0005602409836152521\n",
      "0.11405284848109432      0.11405284849136484\n",
      "0.057090828209371836     0.05709082821341782\n",
      "0.05608292916958746      0.05608292917171707\n",
      "0.05841693152541794      0.0584169315271976\n",
      "0.05523733198842251      0.05523733199251888\n",
      "0.05922148209380751      0.05922148209498508\n",
      "0.16219097626946066      0.16219097631974294\n",
      "0.08120827265534203      0.08120827266195416\n",
      "0.07980574816990327      0.07980574817483962\n",
      "0.08292210186278126      0.08292210186779725\n",
      "0.07879077053463135      0.07879077053878812\n",
      "0.0838936472913332       0.08389364729910642\n",
      "0.23429503631211546      0.23429503625772552\n",
      "0.11704968933656801      0.11704968933036862\n",
      "0.11559660598514654      0.11559660598067678\n",
      "0.1194338210197543       0.11943382101433274\n",
      "0.11414825439093335      0.11414825438703585\n",
      "0.12074157069719149      0.1207415706915175\n",
      "The above two columns you get should be very similar.\n",
      "(Left Col.: Your Numerical Gradient, Right Col.: Analytical Gradient)\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "\n",
      "Relative Difference: 8.3933204208E-11\n"
     ]
    }
   ],
   "source": [
    "checkNNGradients(lambda_reg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============ Part 8: Implement Regularization ============\n",
    "```\n",
    "%  Once your backpropagation implementation is correct, you should now\n",
    "%  continue to implement the regularization with the cost and gradient.\n",
    "%\n",
    "\n",
    "fprintf('\\nChecking Backpropagation (w/ Regularization) ... \\n')\n",
    "\n",
    "%  Check gradients by running checkNNGradients\n",
    "lambda = 3;\n",
    "checkNNGradients(lambda);\n",
    "\n",
    "% Also output the costFunction debugging values\n",
    "debug_J  = nnCostFunction(nn_params, input_layer_size, ...\n",
    "                          hidden_layer_size, num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['\\n\\nCost at (fixed) debugging parameters (w/ lambda = %f): %f ' ...\n",
    "         '\\n(for lambda = 3, this value should be about 0.576051)\\n\\n'], lambda, debug_J);\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Gradient       Analytical Gradient\n",
      "2.8383186911895564e-05   2.838318795382669e-05\n",
      "-0.0452541395024042      -0.04525413950291865\n",
      "0.059802792742313926     0.05980279274399115\n",
      "-0.031871516699144564    -0.03187151669935012\n",
      "0.0008000839057942244    0.0008000839034693133\n",
      "0.050733907510647214     0.05073390751032463\n",
      "-0.05714621645669382     -0.05714621645627034\n",
      "0.024902075257404732     0.0249020752571404\n",
      "-0.002644809895535616    -0.0026448098991406865\n",
      "-0.05797577336430493     -0.057975773363437316\n",
      "0.054800963773526945     0.05480096377166371\n",
      "-0.016210291438056856    -0.01621029143408488\n",
      "-0.002312306377483253    -0.002312306379408663\n",
      "0.05922518599987825      0.059225185999566214\n",
      "-0.04471746532841436     -0.04471746532883083\n",
      "0.009047773499304412     0.009047773499222535\n",
      "-0.0017960278420048326   -0.0017960278440286234\n",
      "-0.06011322503995942     -0.06011322503969079\n",
      "0.039430398703910186     0.03943039870086505\n",
      "0.00955287356552148      0.009552873563392392\n",
      "0.11405284848109432      0.11405284849136484\n",
      "0.04032589831615141      0.04032589832148227\n",
      "0.02388855409041213      0.023888554091690974\n",
      "0.10890519061446113      0.10890519061567139\n",
      "0.09465652791140755      0.09465652791564622\n",
      "0.0844315043035948       0.08443150430458354\n",
      "0.16219097626946066      0.16219097631974294\n",
      "0.1405697674528028       0.14056976745935706\n",
      "0.13924218951189005      0.13924218951653183\n",
      "0.09138930234575682      0.09138930235138928\n",
      "0.103517879650461        0.10351787965329351\n",
      "0.12291091770144646      0.12291091770853343\n",
      "0.23429503631211546      0.23429503625772552\n",
      "0.0844084226825359       0.08440842267700643\n",
      "0.0983224069850408       0.09832240698077287\n",
      "0.061898364540624584     0.061898364534544434\n",
      "0.05414884199694825      0.05414884199399364\n",
      "0.06305772118464503      0.0630577211787441\n",
      "The above two columns you get should be very similar.\n",
      "(Left Col.: Your Numerical Gradient, Right Col.: Analytical Gradient)\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "\n",
      "Relative Difference: 7.6520320949E-11\n",
      "\n",
      "Cost at (fixed) debugging parameters (for lambda = 3, this value should be about 0.576051)\n",
      "\n",
      "For lambda= 3 this cost value will be 0.576051\n"
     ]
    }
   ],
   "source": [
    "# Check gradients by running checkNNGradients\n",
    "lbd = 3\n",
    "checkNNGradients(lbd)\n",
    "# Also output the costFunction debugging values\n",
    "debug_J, grad  = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lbd)\n",
    "\n",
    "print('\\nCost at (fixed) debugging parameters (for lambda = 3, this value should be about 0.576051)\\n')\n",
    "print('For lambda= {} this cost value will be {:.6f}'.format(lbd, debug_J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ================ Part 8: Training NN ================\n",
    "```\n",
    "%  You have now implemented all the code necessary to train a neural \n",
    "%  network. To train your neural network, we will now use \"fmincg\", which\n",
    "%  is a function which works similarly to \"fminunc\". Recall that these\n",
    "%  advanced optimizers are able to train our cost functions efficiently as\n",
    "%  long as we provide them with the gradient computations.\n",
    "%\n",
    "fprintf('\\nTraining Neural Network... \\n')\n",
    "\n",
    "%  After you have completed the assignment, change the MaxIter to a larger\n",
    "%  value to see how more training helps.\n",
    "options = optimset('MaxIter', 50);\n",
    "\n",
    "%  You should also try different values of lambda\n",
    "lambda = 1;\n",
    "\n",
    "% Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = @(p) nnCostFunction(p, ...\n",
    "                                   input_layer_size, ...\n",
    "                                   hidden_layer_size, ...\n",
    "                                   num_labels, X, y, lambda);\n",
    "\n",
    "% Now, costFunction is a function that takes in only one argument (the\n",
    "% neural network parameters)\n",
    "[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);\n",
    "\n",
    "% Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
    "                 hidden_layer_size, (input_layer_size + 1));\n",
    "\n",
    "Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\n",
    "                 num_labels, (hidden_layer_size + 1));\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxiter = 50\n",
    "lambda_reg = 1s\n",
    "myargs = (input_layer_size, hidden_layer_size, num_labels, X, y, lambda_reg)\n",
    "results = minimize(nnCostFunction, x0=nn_params, args=myargs, options={'disp': True, 'maxiter':maxiter}, method=\"L-BFGS-B\", jac=True)\n",
    "\n",
    "nn_params = results[\"x\"]\n",
    "\n",
    "Theta1 = nn_params[0: hidden_layer_size*(input_layer_size+1)].reshape(hidden_layer_size, input_layer_size + 1) #25x401\n",
    "\n",
    "Theta2 = nn_params[Theta1.shape[0]*Theta1.shape[1]:].reshape(num_labels, hidden_layer_size + 1) #10x26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 0.31722116453066795\n",
       " hess_inv: <10285x10285 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([ 2.31088680e-04, -6.79088496e-13,  1.41068089e-13, ...,\n",
       "        1.73932611e-04,  1.31729249e-04,  2.21663569e-05])\n",
       "  message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
       "     nfev: 55\n",
       "      nit: 50\n",
       "   status: 1\n",
       "  success: False\n",
       "        x: array([-6.38865427e-02, -3.39544248e-09,  7.05340443e-10, ...,\n",
       "       -3.51186836e-01,  1.95303935e+00, -1.74870182e+00])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =============== Part 10: Implement Predict ===============\n",
    "```\n",
    "%  After training the neural network, we would like to use it to predict\n",
    "%  the labels. You will now implement the \"predict\" function to use the\n",
    "%  neural network to predict the labels of the training set. This lets\n",
    "%  you compute the training set accuracy.\n",
    "\n",
    "pred = predict(Theta1, Theta2, X);\n",
    "\n",
    "fprintf('\\nTraining Set Accuracy: %f\\n', mean(double(pred == y)) * 100);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 99.24\n"
     ]
    }
   ],
   "source": [
    "pred = predict(Theta1, Theta2, X)\n",
    "print('Training Set Accuracy:', np.mean(pred == y.flatten()) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
