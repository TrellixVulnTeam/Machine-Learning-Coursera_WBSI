{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Online Class \n",
    "# Exercise 3 | Part 1: One-vs-all\n",
    "\n",
    "##  Instructions:\n",
    "```\n",
    "%  ------------\n",
    "%\n",
    "%  This file contains code that helps you get started on the\n",
    "%  linear exercise. You will need to complete the following functions\n",
    "%  in this exericse:\n",
    "%\n",
    "%     lrCostFunction.m (logistic regression cost function)\n",
    "%     oneVsAll.m\n",
    "%     predictOneVsAll.m\n",
    "%     predict.m\n",
    "%\n",
    "%  For this exercise, you will not need to change any code in this file,\n",
    "%  or any other files other than those mentioned above.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ========= Part 1: Loading and Visualizing Data ==========\n",
    "```\n",
    "%  We start the exercise by first loading and visualizing the dataset.\n",
    "%  You will be working with a dataset that contains handwritten digits.\n",
    "%\n",
    "\n",
    "% Load Training Data\n",
    "fprintf('Loading and Visualizing Data ...\\n')\n",
    "\n",
    "load('ex3data1.mat'); % training data stored in arrays X, y\n",
    "m = size(X, 1);\n",
    "\n",
    "% Randomly select 100 data points to display\n",
    "rand_indices = randperm(m);\n",
    "sel = X(rand_indices(1:100), :);\n",
    "\n",
    "displayData(sel);\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "from displayData import displayData\n",
    "data = scipy.io.loadmat('ex3data1.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= data['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = X.shape[0] # 5000x1\n",
    "\n",
    "# y=y.flatten() \n",
    "\n",
    "# # Randomly select 100 data points to display\n",
    "# rand_indices = np.random.permutation(m)\n",
    "# sel = X[rand_indices[:100],:]\n",
    "\n",
    "# displayData(sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ========= Part 2a: Vectorize Logistic Regression =========\n",
    "%  In this part of the exercise, you will reuse your logistic regression\n",
    "%  code from the last exercise. You task here is to make sure that your\n",
    "%  regularized logistic regression implementation is vectorized. After\n",
    "%  that, you will implement one-vs-all classification for the handwritten\n",
    "%  digit dataset.\n",
    "%\n",
    "\n",
    "## matlab code:\n",
    "```\n",
    "% Test case for lrCostFunction\n",
    "fprintf('\\nTesting lrCostFunction() with regularization');\n",
    "\n",
    "theta_t = [-2; -1; 1; 2];\n",
    "X_t = [ones(5,1) reshape(1:15,5,3)/10];\n",
    "y_t = ([1;0;1;0;1] >= 0.5);\n",
    "lambda_t = 3;\n",
    "[J grad] = lrCostFunction(theta_t, X_t, y_t, lambda_t);\n",
    "\n",
    "fprintf('\\nCost: %f\\n', J);\n",
    "fprintf('Expected cost: 2.534819\\n');\n",
    "fprintf('Gradients:\\n');\n",
    "fprintf(' %f \\n', grad);\n",
    "fprintf('Expected gradients:\\n');\n",
    "fprintf(' 0.146561\\n -0.548558\\n 0.724722\\n 1.398003\\n');\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lrCostFunction import lrCostFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cost: 2.534819396109744\n",
      "Expected cost: 2.534819\n",
      "\n",
      "Gradients:\n",
      "\n",
      "0.146561\n",
      "-0.548558\n",
      "0.724722\n",
      "1.398003\n",
      "Expected gradients:\n",
      "\n",
      " 0.146561\n",
      " -0.548558\n",
      " 0.724722\n",
      " 1.398003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "theta_t = np.array([-2, -1, 1, 2])\n",
    "theta_t = theta_t.reshape(-1, 1) # 4x1\n",
    "\n",
    "X_t = np.array([\n",
    "    [1.00000,   0.10000,   0.60000,   1.10000],\n",
    "    [1.00000,   0.20000,   0.70000,  1.20000],\n",
    "    [1.00000,   0.30000,   0.80000,   1.30000],\n",
    "    [1.00000,   0.40000,   0.90000,   1.40000],\n",
    "    [1.00000,   0.50000,   1.00000,   1.50000]\n",
    "])\n",
    "y_t = np.array([1,0,1,0,1]).reshape(-1,1)\n",
    "lambda_t = 3\n",
    "J, grad = lrCostFunction(theta_t, X_t, y_t, lambda_t)\n",
    "\n",
    "print('\\nCost:', J[0][0])\n",
    "print('Expected cost: 2.534819\\n')\n",
    "print('Gradients:\\n')\n",
    "for i in range(4):\n",
    "    print('{:.6f}'.format(grad[i,0]))\n",
    "print('Expected gradients:\\n')\n",
    "print(' 0.146561\\n -0.548558\\n 0.724722\\n 1.398003\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ========== Part 2b: One-vs-All Training ==========\n",
    "```\n",
    "fprintf('\\nTraining One-vs-All Logistic Regression...\\n')\n",
    "\n",
    "lambda = 0.1;\n",
    "[all_theta] = oneVsAll(X, y, num_labels, lambda);\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oneVsAll import oneVsAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 401)\n",
      "(5000, 401)\n",
      "(401, 1)\n",
      "Training 1 out of 10 categories...\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.057333\n",
      "         Iterations: 6\n",
      "         Function evaluations: 23\n",
      "         Gradient evaluations: 39\n",
      "         Hessian evaluations: 0\n",
      "(401, 1)\n",
      "Training 2 out of 10 categories...\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.023975\n",
      "         Iterations: 8\n",
      "         Function evaluations: 25\n",
      "         Gradient evaluations: 65\n",
      "         Hessian evaluations: 0\n",
      "(401, 1)\n",
      "Training 3 out of 10 categories...\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.110819\n",
      "         Iterations: 5\n",
      "         Function evaluations: 23\n",
      "         Gradient evaluations: 34\n",
      "         Hessian evaluations: 0\n",
      "(401, 1)\n",
      "Training 4 out of 10 categories...\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.118972\n",
      "         Iterations: 5\n",
      "         Function evaluations: 23\n",
      "         Gradient evaluations: 34\n",
      "         Hessian evaluations: 0\n",
      "(401, 1)\n",
      "Training 5 out of 10 categories...\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.063622\n",
      "         Iterations: 10\n",
      "         Function evaluations: 29\n",
      "         Gradient evaluations: 71\n",
      "         Hessian evaluations: 0\n",
      "(401, 1)\n",
      "Training 6 out of 10 categories...\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.056203\n",
      "         Iterations: 13\n",
      "         Function evaluations: 17\n",
      "         Gradient evaluations: 285\n",
      "         Hessian evaluations: 0\n",
      "(401, 1)\n",
      "Training 7 out of 10 categories...\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.051579\n",
      "         Iterations: 8\n",
      "         Function evaluations: 27\n",
      "         Gradient evaluations: 51\n",
      "         Hessian evaluations: 0\n",
      "(401, 1)\n",
      "Training 8 out of 10 categories...\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.056174\n",
      "         Iterations: 8\n",
      "         Function evaluations: 28\n",
      "         Gradient evaluations: 54\n",
      "         Hessian evaluations: 0\n",
      "(401, 1)\n",
      "Training 9 out of 10 categories...\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.178629\n",
      "         Iterations: 7\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 51\n",
      "         Hessian evaluations: 0\n",
      "(401, 1)\n",
      "Training 10 out of 10 categories...\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.131604\n",
      "         Iterations: 7\n",
      "         Function evaluations: 26\n",
      "         Gradient evaluations: 51\n",
      "         Hessian evaluations: 0\n"
     ]
    }
   ],
   "source": [
    "num_labels = 10\n",
    "lbd = 0.1\n",
    "all_theta = oneVsAll(X, y, num_labels, lbd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============= Part 3: Predict for One-Vs-All =============\n",
    "\n",
    "```\n",
    "pred = predictOneVsAll(all_theta, X)\n",
    "\n",
    "print('\\nTraining Set Accuracy: %f\\n', mean(double(pred == y)) * 100)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predictOneVsAll import predictOneVsAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Accuracy:\n",
      " 80.78\n"
     ]
    }
   ],
   "source": [
    "pred = predictOneVsAll(all_theta, X)\n",
    "print('\\nTraining Set Accuracy:\\n', np.mean(pred == y.flatten()) * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
